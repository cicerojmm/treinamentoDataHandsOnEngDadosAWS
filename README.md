# Treinamento Engenharia de Dados

Projeto completo de engenharia de dados com pipeline de ingestÃ£o, processamento e anÃ¡lise usando tecnologias AWS e ferramentas open source.

## ğŸ“ PrÃ©-requisitos

- **AWS CLI** configurado com credenciais
- **Terraform** >= 1.0
- **Docker** e **Docker Compose**
- **Git** para versionamento
- **Python** 3.8+ (para scripts utilitÃ¡rios)

### ConfiguraÃ§Ã£o Inicial
```bash
# Clone do repositÃ³rio
git clone <repository-url>
cd treinamentoDataHandsOnEngDadosAWS

# Configurar AWS CLI
aws configure

# Verificar credenciais
aws sts get-caller-identity
```

## ğŸ—ï¸ Arquitetura

### Componentes Principais:
- **PostgreSQL RDS** - Banco de dados transacional
- **DMS Serverless** - ReplicaÃ§Ã£o CDC para S3
- **Debezium + Kafka** - Streaming de mudanÃ§as em tempo real
- **AWS Glue** - ETL e processamento de dados
- **S3 Tables (Iceberg)** - Data Lake com formato Iceberg
- **Step Functions** - OrquestraÃ§Ã£o de workflows
- **OpenSearch** - Busca e analytics em tempo real
- **EventBridge** - Event-driven architecture
- **Metabase** - Business Intelligence e dashboards

## ğŸ“ Estrutura do Projeto

```
treinamentoDataHandsOnEngDadosAWS/
â”œâ”€â”€ terraform/infra/          # Infraestrutura como cÃ³digo
â”‚   â”œâ”€â”€ modules/             # MÃ³dulos Terraform reutilizÃ¡veis
â”‚   â”‚   â”œâ”€â”€ vpc/            # VPC e networking
â”‚   â”‚   â”œâ”€â”€ rds/            # PostgreSQL RDS
â”‚   â”‚   â”œâ”€â”€ dms-serverless/ # DMS para CDC
â”‚   â”‚   â”œâ”€â”€ glue-job/       # Jobs Glue ETL
â”‚   â”‚   â”œâ”€â”€ glue-crawler/   # Crawlers Glue
â”‚   â”‚   â”œâ”€â”€ step-functions/ # OrquestraÃ§Ã£o workflows
â”‚   â”‚   â”œâ”€â”€ lambda_ecr/     # Lambda com Docker
â”‚   â”‚   â”œâ”€â”€ ec2/            # InstÃ¢ncias EC2
â”‚   â”‚   â””â”€â”€ eventbridge/    # Event-driven architecture
â”‚   â”œâ”€â”€ scripts/            # Scripts Glue, Lambda e Step Functions
â”‚   â”‚   â”œâ”€â”€ glue_etl/      # Scripts ETL
â”‚   â”‚   â”œâ”€â”€ lambda_code_ecr/ # CÃ³digo Lambda
â”‚   â”‚   â””â”€â”€ bootstrap/      # Scripts inicializaÃ§Ã£o
â”‚   â”œâ”€â”€ envs/              # ConfiguraÃ§Ãµes por ambiente
â”‚   â””â”€â”€ backends/          # ConfiguraÃ§Ãµes backend Terraform
â”œâ”€â”€ debezium/              # ConfiguraÃ§Ã£o Debezium + Kafka
â”‚   â”œâ”€â”€ connect-configs/   # ConfiguraÃ§Ãµes conectores
â”‚   â””â”€â”€ docker-compose.yml # Stack Kafka/Debezium
â”œâ”€â”€ opensearch/            # Stack OpenSearch
â”œâ”€â”€ metabase/              # Business Intelligence
â”œâ”€â”€ jars/                  # JARs para Spark e Debezium
â”‚   â”œâ”€â”€ debezium/         # JARs Debezium
â”‚   â””â”€â”€ spark-streaming/  # JARs Spark Streaming
â”œâ”€â”€ github-actions/        # CI/CD pipelines
â””â”€â”€ scripts/              # Scripts utilitÃ¡rios
```

## ğŸš€ MÃ³dulos Terraform

### VPC (`modules/vpc`)
- VPC com subnets pÃºblicas e privadas
- Internet Gateway e Route Tables
- VPC Endpoints para S3
- DMS Replication Subnet Group

### RDS PostgreSQL (`modules/rds`)
- InstÃ¢ncia PostgreSQL 17.5
- Security Groups configurados
- Parameter Groups customizados
- Configurado para CDC

### DMS Serverless (`modules/dms-serverless`)
- Source Endpoint (PostgreSQL)
- Target Endpoint (S3)
- Replication Task para CDC
- IAM Roles necessÃ¡rias

### Glue Jobs (`modules/glue-job`)
- Jobs ETL para S3 Tables (Iceberg)
- Processamento de dimensÃµes e fatos
- Data Quality com Great Expectations
- Suporte a JARs customizados

### Step Functions (`modules/step-functions`)
- OrquestraÃ§Ã£o de workflows ETL
- IntegraÃ§Ã£o com Glue Jobs
- Logging e monitoramento

### Lambda ECR (`modules/lambda_ecr`)
- FunÃ§Ãµes Lambda com Docker
- IntegraÃ§Ã£o com DuckDB
- Function URLs configuradas

### EC2 (`modules/ec2`)
- InstÃ¢ncias para processamento
- Security Groups configurados
- Bootstrap scripts

### EventBridge (`modules/eventbridge`)
- Rules para eventos
- IntegraÃ§Ã£o com Lambda
- Event-driven architecture

### Glue Crawler (`modules/glue-crawler`)
- Descoberta automÃ¡tica de schemas
- CatalogaÃ§Ã£o de dados S3
- IntegraÃ§Ã£o com Glue Data Catalog

## ğŸ”§ Deploy da Infraestrutura

### 1. Configurar Backend
```bash
cd terraform/infra
terraform init -backend-config="backends/develop.hcl"
```

### 2. Aplicar Terraform
```bash
terraform apply -var-file=envs/develop.tfvars -auto-approve
```

### 3. Iniciar ReplicaÃ§Ã£o DMS
```bash
./modules/dms-serverless/start_replication.sh dev
```

## ğŸ“Š Pipeline de Dados

### 1. IngestÃ£o
- **DMS**: Replica dados do PostgreSQL para S3 (full-load + CDC)
- **Debezium**: Captura mudanÃ§as em tempo real via Kafka

### 2. Processamento
- **Glue ETL**: Transforma dados raw em dimensÃµes e fatos
- **S3 Tables**: Armazena dados em formato Iceberg
- **Step Functions**: Orquestra pipeline ETL

### 3. Consumo
- **OpenSearch**: Analytics e busca em tempo real
- **Lambda + DuckDB**: Queries analÃ­ticas
- **S3 Tables**: Consultas SQL diretas

## ğŸ”„ ConfiguraÃ§Ã£o CDC

### RDS PostgreSQL
```sql
-- ConfiguraÃ§Ãµes necessÃ¡rias no Parameter Group
rds.logical_replication = 1
max_replication_slots = 10
max_wal_senders = 10
```

### Debezium + Kafka
```bash
cd debezium
docker-compose up -d
```

### Configurar Conectores
```bash
# Conector PostgreSQL para web_events
curl -X POST http://localhost:8083/connectors \
  -H "Content-Type: application/json" \
  -d @connect-configs/postgres.json

# Verificar status
curl http://localhost:8083/connectors/postgres-connector-ecommerce-final/status
```

## ğŸ§ª S3 Tables Iceberg

### Configurar Connector
1. Build do connector oficial:
```bash
./debezium/build-iceberg-s3-connect.sh
```

2. Configurar JARs:
```bash
# JARs Debezium (Avro/Schema Registry)
cp jars/debezium/* kafka-connect-debezium/

# JARs do Iceberg
cp /iceberg/kafka-connect/kafka-connect-runtime/build/distributions/* kafka-connect-confluent
```


### JARs IncluÃ­dos

#### Debezium (`jars/debezium/`)
- `kafka-avro-serializer-8.0.0.jar`
- `kafka-connect-avro-converter-8.0.0.jar`
- `kafka-schema-registry-client-8.0.0.jar`

#### Spark Streaming (`jars/spark-streaming/`)
- `opensearch-spark-30_2.12-1.3.0.jar`
- `spark-sql-kafka-0-10_2.12-3.3.4.jar`
- `spark-avro_2.12-3.3.4.jar`
- `kafka-clients-3.5.2.jar`

## ğŸ” OpenSearch

### Deploy
```bash
cd opensearch
docker-compose up -d
```

### Acesso
- **URL**: http://localhost:5601
- **UsuÃ¡rio**: admin
- **Senha**: admin

## ğŸ“Š Metabase

### Deploy
```bash
cd metabase
docker-compose up -d
```

### Acesso
- **URL**: http://localhost:3000
- **ConfiguraÃ§Ã£o**: Primeira execuÃ§Ã£o requer setup inicial
- **Banco**: PostgreSQL interno para metadados

## ğŸ“ Scripts DisponÃ­veis

### Glue ETL
- `datahandson-engdados-amazonsales-dw-table-stg-s3tables.py` - Staging
- `datahandson-engdados-amazonsales-dw-dim-*.py` - DimensÃµes
- `datahandson-engdados-amazonsales-dw-fact-*.py` - Fatos
- `*-gdq.py` - Data Quality

### Spark Streaming
- `datahandson-engdados-webevents-streaming-kafka-opensearch.py` - Streaming Kafka â†’ OpenSearch

### Lambda
- `lambda_handler.py` - Queries analÃ­ticas com DuckDB
- `build_and_push.sh` - Build e deploy container ECR
- `Dockerfile` - Container Lambda

### UtilitÃ¡rios
- `script-insert-postgres-webfake-events.py` - GeraÃ§Ã£o de dados fake
- `ec2_bootstrap.sh` - InicializaÃ§Ã£o instÃ¢ncias EC2

## ğŸ› ï¸ Monitoramento

### DMS
```bash
./modules/dms-serverless/monitor_replication.sh dev
```

### Step Functions
- Console AWS Step Functions
- CloudWatch Logs

### Glue Jobs
- Console AWS Glue
- CloudWatch Metrics

### Kafka + Debezium
```bash
# Status conectores
curl http://localhost:8083/connectors

# Logs
docker-compose logs -f connect
```

## ğŸ”§ Troubleshooting

### Debezium
```bash
# Verificar conectores
curl http://localhost:8083/connectors

# Status detalhado
curl http://localhost:8083/connectors/postgres-connector-ecommerce-final/status

# Reiniciar conector
curl -X POST http://localhost:8083/connectors/postgres-connector-ecommerce-final/restart

# Logs do Kafka Connect
docker-compose logs -f connect
```

### Kafka Topics
```bash
# Listar tÃ³pics
docker exec kafka kafka-topics --bootstrap-server localhost:9092 --list

# Consumir mensagens
docker exec kafka kafka-console-consumer \
  --bootstrap-server localhost:9092 \
  --topic ecommercefinal.public.web_events \
  --from-beginning
```

### Glue Jobs
```bash
# Logs CloudWatch
aws logs describe-log-groups --log-group-name-prefix "/aws-glue/jobs"

# Status job
aws glue get-job-run --job-name <job-name> --run-id <run-id>
```

## ğŸ“‹ VariÃ¡veis de Ambiente

### Desenvolvimento (`envs/develop.tfvars`)
```hcl
environment = "dev"
region = "us-east-2"
s3_bucket_raw = "cjmm-mds-lake-raw"
s3_bucket_scripts = "cjmm-mds-lake-configs"
s3_bucket_curated = "cjmm-mds-lake-curated"
```

## ğŸ” SeguranÃ§a

- IAM Roles com princÃ­pio de menor privilÃ©gio
- Security Groups restritivos
- VPC Endpoints para comunicaÃ§Ã£o privada
- Criptografia em trÃ¢nsito e repouso

## ğŸ”„ CI/CD com GitHub Actions

### Pipeline Terraform (`github-actions/terraform.yml`)
- **Triggers**: Pull requests e pushes para `develop` e `main`
- **Ambientes**: AutomÃ¡tico baseado na branch
  - `develop` â†’ ambiente `dev`
  - `main` â†’ ambiente `prod`
- **Steps**:
  - Terraform fmt, validate e plan
  - Apply automÃ¡tico em push para branches principais
  - Upload de artifacts do plan em PRs

### ConfiguraÃ§Ã£o
```yaml
# Secrets necessÃ¡rios no GitHub:
AWS_ACCESS_KEY_ID
AWS_SECRET_ACCESS_KEY
```

### Uso
1. **Pull Request**: Executa plan e valida cÃ³digo
2. **Merge**: Aplica mudanÃ§as automaticamente
3. **Artifacts**: Plan disponÃ­vel para review

## ğŸ¯ Casos de Uso

### 1. E-commerce Analytics
- AnÃ¡lise de vendas por categoria
- Ratings de produtos
- Comportamento de usuÃ¡rios

### 2. Real-time Streaming
- Eventos web em tempo real
- Processamento Kafka â†’ OpenSearch
- Dashboards interativos

### 3. Data Quality
- ValidaÃ§Ã£o com Great Expectations
- Monitoramento de qualidade
- Alertas automÃ¡ticos

## ğŸ“š Tecnologias

- **AWS**: RDS, DMS, Glue, Step Functions, Lambda, S3, EventBridge
- **Terraform**: Infraestrutura como cÃ³digo
- **GitHub Actions**: CI/CD pipeline
- **Apache Kafka**: Streaming de dados
- **Debezium**: Change Data Capture
- **Apache Iceberg**: Table format para Data Lake
- **OpenSearch**: Search e analytics
- **Metabase**: Business Intelligence
- **DuckDB**: Analytics engine
- **Docker**: ContainerizaÃ§Ã£o